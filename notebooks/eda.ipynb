{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Classification Task: Breast Cancer Wisconsin Diagnostics Dataset</center></h1><br />\n",
    "<h2>Otto von Sperling<h2/>\n",
    "<h3>December 16th, 2019 <h3/><br />\n",
    "<h2><center>Data Scientist Application</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.realpath(__file__).rsplit('notebooks', 1)[0])",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.eda import BCW_Explorer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "eda = BCW_Explorer()\n",
    "data = eda.data.drop(('label', ''), axis=1)\n",
    "labels = eda.data.droplevel(1, axis=1)['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "eda.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# 2. Data Analysis\n",
    "\n",
    "## 2.1 Overview of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    'Attributes:\\n\\t{0}\\n'.format(\n",
    "        ', '.join(str(x) for x in eda.data.columns.get_level_values('attributes').unique() if x)\n",
    "    ),\n",
    "    '\\n... where each attribute is summarized by:\\n\\t{0}'.format(\n",
    "        ', '.join(str(x) for x in eda.data.columns.get_level_values('summaries').unique() if x != 'label')\n",
    "    ),\n",
    "    '\\n\\n... for a total of 30 features'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "At first, we should assume that there is correlation between some of the features given the nature of the attributes which they summarize.\n",
    "* radius yields (although the attribute is the mean of distances from center to points on the perimeter)\n",
    "    * perimiter, area, smoothness (local variation in radius lengths) and compactness (perimeter^2 / area - 1.0);\n",
    "\n",
    "We explore whether they significantly correlate in the section 2.3\n",
    "<br />\n",
    "\n",
    "\\> **What is the attribute Fractal Dimension?**<br />\n",
    "&emsp;Fractal dimension is a measure of how regular the contour of a shape is.<br />\n",
    "It is approximated by measuring the downward slope of the log of observed perimeter plotted agains the log of a \"ruler\" size.<br />\n",
    "Hence, a higher value corresponds to a less regular contour, which in turn means a higher probability of malignancy (Wolberg et al. 1994)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Research Questions (RQs)\n",
    "\n",
    "1. What are the most significant features, and how to measure significance?\n",
    "2. Which classifiers are most suitable for the task, and how to decide?\n",
    "3. What performance measure should be favored during hyperparameter tuning?\n",
    "4. How much better (or worse) are classifiers induced from reduced dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Class Imbalance, Feature Distribution and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.set_title('Balance of Classes', fontsize=14)\n",
    "\n",
    "eda.data['label']\\\n",
    "    .map({1: 'Malignant', 0: 'Benign'})\\\n",
    "    .value_counts()\\\n",
    "    .plot(\n",
    "        ax=ax,\n",
    "        kind='pie',\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=270,\n",
    "        fontsize=14,\n",
    "        label=''\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### - Imbalanced Classes\n",
    "&emsp;There are almost twice as many benign observations than malignant, which can become an issue while inducing machine learning models. We can either employ a sampling technique or adjust our assessment of models to account for such imbalance. The former entails taking advantage of techniques such as SMOTE to generate new samples that follow the distribution of a give class, the latter means going beyond accuracy measures and throroughly assessing the assumptions and aims of other methods such as Confusion Matrix and Reliability Diagrams.<br />\n",
    "<br />\n",
    "\n",
    "<!-- <center><img style=\"margin-bottom:5mm\" src=\"https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png\"><center/>\n",
    "<center>Figure 1. Approaches to resampling<center/>\n",
    "<center style=\"margin-bottom:5mm\">Source. https://tinyurl.com/yy2qtbe2<center/>\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_sample(data.values, labels.values)\n",
    "\n",
    "f = plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_2d_space(data.values, labels.values, 'No over-sampling')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For now, we choose to employ better assessment methods so as to avoid injecting bias into the dataset due to the delicate nature of this study.\n",
    "\n",
    "&emsp;Before proceeding to the classifier induction task, we investigate the distribution of our features, comprised of some **statistical analysis** and **data visualization**, and whether there are any redundant candidates for removal via **correlation clustering**.<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox, skew\n",
    "\n",
    "def plot_compare_classdistribution(in_df: pd.DataFrame, summary=None, ax=None, feats=None):\n",
    "    '''\n",
    "    '''\n",
    "    try:\n",
    "        df = in_df[feats].apply(preprocessing.scale, axis=0).copy()\n",
    "    \n",
    "    except KeyError:\n",
    "        df = in_df.apply(preprocessing.scale, axis=0).copy()\n",
    "\n",
    "    try:\n",
    "        data = df.xs(summary, level='summaries', axis=1)\n",
    "    \n",
    "    except KeyError:\n",
    "        data = df\n",
    "\n",
    "    # melts columns' (name, value) pair into two columns\n",
    "    df =\\\n",
    "        pd.melt(\n",
    "            pd.concat(\n",
    "                [\n",
    "                    eda.data.droplevel(1, axis=1)['label'],\n",
    "                    data\n",
    "                ],\n",
    "                axis=1\n",
    "            ),\n",
    "            id_vars='label',\n",
    "            var_name='features',\n",
    "            value_name='value'\n",
    "        )\n",
    "\n",
    "    # plots comparative distribution of classes for each 'mean' feature\n",
    "    return\\\n",
    "        sns.violinplot(\n",
    "            ax=ax,\n",
    "            data=df,\n",
    "            y=\"value\",\n",
    "            x=\"features\",\n",
    "            hue=\"label\",\n",
    "            inner=\"quartile\",\n",
    "            split=True,\n",
    "        )\n",
    "\n",
    "\n",
    "def transform_skew(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Transforms data with Boxcox transformation\n",
    "    Keeps the transformation only if it is less skewed than the original\n",
    "    '''\n",
    "    skew_transformed = pd.DataFrame()\n",
    "\n",
    "    for feat in df:\n",
    "\n",
    "        data = df[feat].values\n",
    "        posdata = data[data > 0]\n",
    "        # posdata = eda.data[eda.data[feat]>0][feat]\n",
    "\n",
    "        x, lmbda = boxcox(posdata, lmbda=None)\n",
    "        \n",
    "        transform = np.empty_like(data)\n",
    "        transform[data > 0] = x\n",
    "        transform[data == 0] = -1/lmbda\n",
    "\n",
    "        if abs(skew(transform)) < abs(skew(data)):\n",
    "            skew_transformed[feat] = transform\n",
    "\n",
    "        else:\n",
    "            skew_transformed[feat] = data\n",
    "    \n",
    "    skew_transformed.columns =\\\n",
    "        pd.MultiIndex.from_tuples(\n",
    "            list(skew_transformed.columns),\n",
    "            names=('summaries', 'attributes')\n",
    "        )\n",
    "\n",
    "    return skew_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "\n",
    "ax.set_title('Comparative Distribution of Features (standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(data, ax=ax, summary='mean')\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "leg = ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "print('Skewness of features:')\n",
    "eda.data.skew().sort_values(ascending=False).sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Applies boxcox transformation')\n",
    "transformed = transform_skew(data)\n",
    "transformed['label'] = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "\n",
    "ax.set_title('Comparative Distribution of Features (skewed transformed, standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(transformed, summary='mean')\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "print('Skewness after Boxcox transform:')\n",
    "transformed.skew().sort_values(ascending=False).sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_unique_correlation(data: pd.DataFrame, threshold=0.5) -> pd.DataFrame:\n",
    "    try:\n",
    "        tmp = data.drop('label', axis=1).copy()\n",
    "    \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    tmp.columns = tmp.columns.to_flat_index()\n",
    "    dataCorr = tmp.corr(method='pearson')\n",
    "\n",
    "    dataCorr = dataCorr.mask(np.tril(np.ones(dataCorr.shape)).astype(np.bool))\n",
    "    dataCorr = dataCorr[abs(dataCorr) >= threshold].stack()\n",
    "\n",
    "    dataCorr = dataCorr.reset_index(level=0)\n",
    "    dataCorr.index = pd.MultiIndex.from_tuples(dataCorr.index, names=('summaries', 'attributes'))\n",
    "    \n",
    "    return dataCorr.sort_index(level='attributes').sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(17, 12))\n",
    "ax.set_title('Correlation Matrix', fontsize=14)\n",
    "\n",
    "corr = data.corr()\n",
    "\n",
    "# generates an upper diagonal mask\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    ax=ax,\n",
    "    vmax=1,\n",
    "    cmap=cmap,\n",
    "    mask=mask,\n",
    "    annot=False,\n",
    "    square=False,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "**Feature Selection Stategy**<br />\n",
    "&emsp;Before applying an automated search for the optimal dimensionality for each classifer, we investigate features we consider relevant for the sake of comparison. There are a few features (variables) that correlate with many others, which could lead to using them to represent all others. At the end of this process, we will be able to answer **RQ1**.\n",
    "* What are the most significant features, and how to measure significance? (RQ1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "**Correlation clustering** is the task of ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def correlation_clustering(corr: np.ndarray, labels=None, threshold=0.5, ax=plt.axes, maxclust=None) -> (dict or np.ndarray):\n",
    "    '''\n",
    "    '''\n",
    "    dissimilarity = 1 - np.abs(corr)\n",
    "    hierarchy = linkage(squareform(dissimilarity), method='ward', metric='distance')\n",
    "    \n",
    "    return\\\n",
    "        (\n",
    "            fcluster(hierarchy, maxclust, criterion='maxclust') if maxclust\n",
    "            else\n",
    "            dendrogram(\n",
    "                hierarchy, color_threshold=None,\n",
    "                labels=labels, ax=ax,\n",
    "                distance_sort='ascending',\n",
    "                leaf_font_size=12,\n",
    "                orientation='right',\n",
    "            )\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "ax.set_title('Correlation Clustering', fontsize=14)\n",
    "\n",
    "d = correlation_clustering(corr.values, corr.index.values, ax=ax)\n",
    "\n",
    "# plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# groups features into their respective clusters\n",
    "cluster = correlation_clustering(corr.values, labels=corr.index.values, maxclust=3)\n",
    "cluster =\\\n",
    "    pd.Series(cluster, corr.index.values, name='cluster')\\\n",
    "        .reset_index().groupby('cluster')['index'].apply(np.array)\\\n",
    "        .to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "@interact\n",
    "def viz_cluster_separation(\n",
    "    feature1=[pair[0] +'_'+ pair[1] for pair in cluster[1]],\n",
    "    feature2=[pair[0] +'_'+ pair[1] for pair in cluster[2]],\n",
    "    feature3=[pair[0] +'_'+ pair[1] for pair in cluster[3]]\n",
    "):\n",
    "    '''\n",
    "    '''\n",
    "    feature1 = tuple(feature1.split('_', 1))\n",
    "    feature2 = tuple(feature2.split('_', 1))\n",
    "    feature3 = tuple(feature3.split('_', 1))\n",
    "\n",
    "    fig =\\\n",
    "        make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            specs=[\n",
    "                [{'is_3d': True}, {'is_3d': True}]\n",
    "            ],\n",
    "            print_grid=False,\n",
    "            subplot_titles=('Original features', 'Standardized features')\n",
    "        )\n",
    "\n",
    "    x = data[feature1]\n",
    "    y = data[feature2]\n",
    "    z = data[feature3]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode='markers',\n",
    "            showlegend=False,\n",
    "            \n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=labels.values,\n",
    "                colorscale=['#CC8963', '#5875A4'],\n",
    "                opacity=1.0\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    x = transformed[feature1]\n",
    "    y = transformed[feature2]\n",
    "    z = transformed[feature3]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode='markers',\n",
    "            showlegend=False,\n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=labels.values,\n",
    "                colorscale=['#CC8963', '#5875A4'],\n",
    "                opacity=1.0\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        margin=dict(r=10, l=10, b=10, t=10),\n",
    "        title_text='Class separation',\n",
    "        scene = dict(\n",
    "            xaxis_title=feature1[0] +'_'+ feature1[1],\n",
    "            yaxis_title=feature2[0] +'_'+ feature2[1],\n",
    "            zaxis_title=feature3[0] +'_'+ feature3[1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most significant features, and how to measure significance? **(RQ1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "print('Selects the feature that has highest correlation to target variable (\"label\") within each cluster.')\n",
    "target = eda.data.corr()[('label', '')]\n",
    "selected_feats = [target[cluster[k]].idxmax() for k in cluster.keys()]\n",
    "selected_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.set_title('Distribution of selected features (skewed tranformed and standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(transformed, ax=ax, feats=selected_feats)\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "leg = ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splits data and labels\n",
    "X = data.values\n",
    "y = labels.values\n",
    "\n",
    "# # splits train and test\n",
    "# X_train, X_test, y_train, y_test =\\\n",
    "#     train_test_split(\n",
    "#         X,\n",
    "#         y,\n",
    "#         test_size=.2,\n",
    "#         random_state=42\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Selection and Induction\n",
    "Which classifiers are most suitable for the task, and how to decide? **(RQ2)**\n",
    "\n",
    "When choosing a solution to a problem, one must keep in mind the principle of Occam's Razor, which states that given two equally effective solutions, the simpler one is more likely to be the correct one. ~~Thus, having visually observed that our selected features nearly separate the classes linearly, we choose linear models for classification. However we must also test this assumption (classes are linearly separable) so that we have more confidence in our classification. We test our assumption by training linear kernel Support Vector Machine Classifier. If such classifier achieves perfect accuracy (100%), it means that our classes are linearly separable~~. Classes are not entirely linearly separable. Still, we keep the test in this document as evidence of it.\n",
    "\n",
    "### - Test linear separability of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-- Convex Hulls and Linear Programming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Drawing Convex Hulls to test for intersection \n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "feats = [x[0]+'_'+x[1] for x in eda.data[selected_feats].columns.to_flat_index()]\n",
    "\n",
    "@interact\n",
    "def convex_hulls_separability(\n",
    "    feature1=feats,\n",
    "    feature2=feats[::-1]\n",
    "):\n",
    "    if feature1 != feature2:\n",
    "        feats = [tuple(x.split('_', 1)) for x in [feature1, feature2]]\n",
    "        d = eda.data[feats].copy()\n",
    "        d.columns = d.columns.to_flat_index()\n",
    "        d['label'] = labels.values\n",
    "        label = labels.values\n",
    "\n",
    "        plt.clf()\n",
    "        plt.figure(figsize = (20, 6))\n",
    "        names = ['Benign', 'Malignant']\n",
    "        colors = ['#CC8963', '#5875A4']\n",
    "        \n",
    "        plt.title('{0} vs. {1}'.format(feature1.upper(), feature2.upper()))\n",
    "        plt.xlabel(feature1)\n",
    "        plt.ylabel(feature2)\n",
    "\n",
    "        for i in range(len(names)):\n",
    "            bucket = d[d['label'] == i]\n",
    "            bucket = bucket.iloc[:,[0,1]].values\n",
    "            hull = ConvexHull(bucket)\n",
    "            plt.scatter(bucket[:, 0], bucket[:, 1], label=names[i]) \n",
    "            for j in hull.simplices:\n",
    "                plt.plot(bucket[j,0], bucket[j,1], colors[i])\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        d[\"newlabel\"] = np.where(d['label'] == 0, 1 , -1)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        tmp = d.iloc[:,[0,1]].values\n",
    "        tmp = sc.fit_transform(tmp)\n",
    "\n",
    "        xx = np.array(d['newlabel'].values.reshape(-1,1) * tmp)\n",
    "        t = np.where(d['label'] == 0, 1 , -1)\n",
    "\n",
    "        #2-D array which, when matrix-multiplied by x, gives the values of \n",
    "        #the upper-bound inequality constraints at x.\n",
    "        A_ub = np.append(xx, t.reshape(-1,1), 1)\n",
    "\n",
    "        #1-D array of values representing the upper-bound of each \n",
    "        #inequality constraint (row) in A_ub.\n",
    "        b_ub = np.repeat(-1, A_ub.shape[0]).reshape(-1,1)\n",
    "\n",
    "        # Coefficients of the linear objective function to be minimized.\n",
    "        c_obj = np.repeat(1, A_ub.shape[1])\n",
    "        res = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub,\n",
    "                      options={\"disp\": False})\n",
    "\n",
    "        if res.success:\n",
    "            print('There is linear separability between Benign and Malignant via \"{}\" and \"{}\"'.format(feature1, feature2))\n",
    "        else:\n",
    "            print('No linear separability between Benign and Malignant via \"{}\" and \"{}\"'.format(feature1, feature2))\n",
    "    \n",
    "    else:\n",
    "        print('Please, select two distinct features to draw hulls and check for separability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "del feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;This test show us that although nearly, the classes are not fully linearly separable. This informs our choice of classifiers, which will favor non-linear ones such as RBF SVM, MLP, Random Forest and so on. Next, we induce our models and define metrics for comparison between models and data dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# class ClfSwitcher(BaseEstimator):\n",
    "#     '''\n",
    "#     Boilerplate class to support hyperparameter tunning for multiple classifiers\n",
    "#     '''\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         estimator = SGDClassifier(),\n",
    "#     ): \n",
    "#         self.estimator = estimator\n",
    "\n",
    "\n",
    "#     def fit(self, X, y=None, **kwargs):\n",
    "#         self.estimator.fit(X, y)\n",
    "#         return self\n",
    "\n",
    "\n",
    "#     def predict(self, X, y=None):\n",
    "#         return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "#     def predict_proba(self, X):\n",
    "#         return self.estimator.predict_proba(X)\n",
    "\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         return self.estimator.score(X, y)\n",
    "\n",
    "#     def decision_function(self, X):\n",
    "#         return self.estimator.decision_function(X)\n",
    "\n",
    "\n",
    "\n",
    "class Correlation_Clustering(TransformerMixin, BaseEstimator):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.n_components:\n",
    "            X = pd.DataFrame(X)\n",
    "            cluster = correlation_clustering(\n",
    "                X.corr().values, maxclust=self.n_components\n",
    "            )\n",
    "\n",
    "            X['label'] = y\n",
    "            self.corr = X.corr()\n",
    "            self.corr = self.corr.drop('label', axis=0)\n",
    "            self.corr['cluster'] = cluster\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Apply dimensionality reduction to X via correlation clustering\n",
    "        NOT OPTMIZED AT ALL!!\n",
    "        '''\n",
    "        try:\n",
    "            sel =\\\n",
    "                self.corr.groupby('cluster')['label']\\\n",
    "                    .apply(lambda x: x.idxmax()).values\n",
    "        \n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        return X[:, sel] if self.n_components else X\n",
    "\n",
    "\n",
    "\n",
    "class Skew_Scale(TransformerMixin, BaseEstimator):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        result = np.empty_like(X)\n",
    "        for col, i in zip(X.T, range(len(X.T))):\n",
    "            y, lmbda = boxcox(col[col > 0], lmbda=None)\n",
    "            transform = np.empty_like(col)\n",
    "            transform[col > 0] = y\n",
    "            transform[col == 0] = -1/lmbda\n",
    "\n",
    "            if abs(skew(transform)) < abs(skew(col)):\n",
    "                result.T[i] = transform\n",
    "            else:\n",
    "                result.T[i] = col\n",
    "\n",
    "        return preprocessing.scale(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('preprocessing', 'passthrough'),\n",
    "#     ('reduce_dim', 'passthrough'),\n",
    "#     ('clf', ClfSwitcher()),\n",
    "# ])\n",
    "\n",
    "# param_grid =  [\n",
    "#     {\n",
    "#         'clf__estimator': [SVC()],\n",
    "# #         'preprocessing': [Skew_Scale()],\n",
    "#         'reduce_dim': [Correlation_Clustering()],\n",
    "#         'reduce_dim__n_components': [3, 5],\n",
    "#         #---------------------------------------------------\n",
    "#         'clf__estimator__kernel': ['rbf'],\n",
    "#         'clf__estimator__gamma': ['scale'],\n",
    "#         'clf__estimator__tol': [1e-3, 1e-2, 1e-1],\n",
    "#         'clf__estimator__C': [1.0, 50, 1e2],\n",
    "#         'clf__estimator__max_iter': [-1],\n",
    "#         'clf__estimator__class_weight': ['balanced', None]\n",
    "#     },\n",
    "#     {\n",
    "#         'clf__estimator': [RandomForestClassifier()],\n",
    "# #         'preprocessing': [Skew_Scale()],\n",
    "#         'reduce_dim': [Correlation_Clustering()],\n",
    "#         'reduce_dim__n_components': [3, 5],\n",
    "#         #---------------------------------------------------\n",
    "#         'clf__estimator__criterion': ['gini'],\n",
    "#         'clf__estimator__min_samples_split': [3, 5, 7],\n",
    "#         'clf__estimator__min_impurity_decrease': [1e-1],\n",
    "#         'clf__estimator__n_estimators': [200, 250],\n",
    "#         'clf__estimator__oob_score': [False],\n",
    "#         'clf__estimator__class_weight': ['balanced', 'balanced_subsample'],\n",
    "#         'clf__estimator__ccp_alpha': [1e-1],\n",
    "#         'clf__estimator__bootstrap': [True],\n",
    "#         'clf__estimator__max_samples': [.1, .15, .2],\n",
    "#         'clf__estimator__max_leaf_nodes': [100, 200, 300],\n",
    "#         'clf__estimator__min_samples_leaf': [1, 3],\n",
    "#     },\n",
    "#     {\n",
    "#         'clf__estimator': [MLPClassifier()],\n",
    "#         #         'preprocessing': [Skew_Scale()],\n",
    "#         'reduce_dim': [Correlation_Clustering()],\n",
    "#         'reduce_dim__n_components': [3, 5],\n",
    "#         #---------------------------------------------------\n",
    "#         'clf__estimator__hidden_layer_sizes': [(100, ), (100, 2)],\n",
    "#         'clf__estimator__activation': ['relu'],\n",
    "#         'clf__estimator__solver': ['adam'],\n",
    "#         'clf__estimator__alpha': [0.0001],\n",
    "#         'clf__estimator__batch_size': ['auto'],\n",
    "#         'clf__estimator__learning_rate': ['constant'],\n",
    "#         'clf__estimator__learning_rate_init': [0.001],\n",
    "#         'clf__estimator__power_t': [0.5],\n",
    "#         'clf__estimator__max_iter': [200],\n",
    "#         'clf__estimator__shuffle': [True],\n",
    "#         'clf__estimator__random_state': [None],\n",
    "#         'clf__estimator__tol': [0.0001],\n",
    "#         'clf__estimator__verbose': [False],\n",
    "#         'clf__estimator__warm_start': [False],\n",
    "#         'clf__estimator__momentum': [0.9],\n",
    "#         'clf__estimator__nesterovs_momentum': [True],\n",
    "#         'clf__estimator__early_stopping': [False],\n",
    "#         'clf__estimator__validation_fraction': [0.1],\n",
    "#         'clf__estimator__beta_1': [0.9],\n",
    "#         'clf__estimator__beta_2': [0.999],\n",
    "#         'clf__estimator__epsilon': [1e-08],\n",
    "#         'clf__estimator__n_iter_no_change': [10],\n",
    "#         'clf__estimator__max_fun': [15000]\n",
    "#     },\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# # scales data\n",
    "# ss = Skew_Scale()\n",
    "# ss.fit(X, y)\n",
    "# X = ss.transform(X)\n",
    "\n",
    "# # fits models in pipeline\n",
    "# gscv = GridSearchCV(\n",
    "#     pipeline, param_grid,\n",
    "#     scoring=['f1_weighted', 'roc_auc'],\n",
    "#     refit='roc_auc',\n",
    "#     cv=5, n_jobs=-1, verbose=3\n",
    "# )\n",
    "# gscv.fit(X, y)\n",
    "# # gscv.fit(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (gscv.best_score_, gscv.best_params_), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-- Final choice of models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "clf = {\n",
    "    'rf': RandomForestClassifier(\n",
    "        bootstrap=True, ccp_alpha=0.01, class_weight='balanced',\n",
    "        criterion='gini', max_depth=None, max_features='auto',\n",
    "        max_leaf_nodes=100, max_samples=0.15,\n",
    "        min_impurity_decrease=0.1, min_impurity_split=None,\n",
    "        min_samples_leaf=1, min_samples_split=3,\n",
    "        min_weight_fraction_leaf=0.0, n_estimators=250,\n",
    "        n_jobs=None, oob_score=False, random_state=None,\n",
    "        verbose=0, warm_start=False\n",
    "    ),\n",
    "    'svm': SVC(\n",
    "        C=100.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "        max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.01,\n",
    "        verbose=False\n",
    "    ),\n",
    "    'mlp': MLPClassifier(\n",
    "        activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "        beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "        hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "        learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
    "        momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "        power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
    "        tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "        warm_start=False\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Assessing Results\n",
    "What performance measure should be favored during hyperparameter tuning? **(RQ3)**\n",
    "\n",
    "How much better (or worse) are classifiers induced from reduced dimensionality? **(RQ4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# realoads the data fresh\n",
    "X_train = data.values\n",
    "y_train = labels.values\n",
    "\n",
    "X_test = eda.holdout.drop(('label', ''), axis=1).values\n",
    "y_test = eda.holdout.droplevel(1, axis=1)['label'].values\n",
    "\n",
    "# transforms skew and scales\n",
    "ss = Skew_Scale()\n",
    "ss.fit(X_train, y_train)\n",
    "X_train = ss.transform(X_train)\n",
    "ss.fit(X_test, y_test)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "# reduces dimensionality\n",
    "reduce_dim = Correlation_Clustering(n_components=3)\n",
    "reduce_dim.fit(X_train, y_train)\n",
    "x_train = reduce_dim.transform(X_train)\n",
    "reduce_dim.fit(X_test, y_test)\n",
    "x_test = reduce_dim.transform(X_test)\n",
    "\n",
    "# fits classifiers to the train data\n",
    "[clf[model].fit(X_train, y_train) for model in clf.keys()]\n",
    "\n",
    "# generates predictions\n",
    "y_pred = {\n",
    "    model: clf[model].predict(X_test)\n",
    "    for model in clf.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc,precision_recall_curve,roc_curve\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_test: list, y_pred: list,\n",
    "    benign_count: int, malignant_count: int,\n",
    "#     axes: plt.axes\n",
    "):\n",
    "    '''\n",
    "    '''\n",
    "    cfn_matrix = confusion_matrix(y_test, y_pred)\n",
    "    cfn_norm_matrix = np.array(\n",
    "        [\n",
    "            [1.0 / benign_count, 1.0 / benign_count],\n",
    "            [1.0 / malignant_count, 1.0 / malignant_count]\n",
    "        ]\n",
    "    )\n",
    "    norm_cfn_matrix = cfn_matrix * cfn_norm_matrix\n",
    "\n",
    "    plt.ylabel('Real Classes')\n",
    "    plt.xlabel('Predicted Classes')\n",
    "#     plt.tight_layout()\n",
    "\n",
    "    \n",
    "    return sns.heatmap(norm_cfn_matrix,cmap='coolwarm_r',linewidths=0.5,annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "for model, index in zip(list(clf.keys()), range(1, (len(clf.keys())+1))):\n",
    "    fig.add_subplot(2,2,index)\n",
    "    plt.title('Confusion Matrix ('+model.upper()+')')\n",
    "    plot_confusion_matrix(y_test, y_pred[model], (y_test == 0).sum(), (y_test == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "### - Reliability Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.calibration import calibration_curve\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# # predict uncalibrated probabilities\n",
    "# def uncalibrated(X_train, X_test, y_train, model):\n",
    "#     # predict probabilities\n",
    "#     try:\n",
    "#         result = clf[model].predict_proba(X_test)[:, 1]\n",
    "#     except AttributeError:\n",
    "#         result = clf[model].decision_function(X_test)\n",
    "    \n",
    "#     return result\n",
    " \n",
    "# # predict calibrated probabilities\n",
    "# def calibrated(X_train, X_test, y_train):\n",
    "#     # define and fit calibration model\n",
    "#     calibrated = CalibratedClassifierCV(clf['svm'], method='sigmoid', cv=5)\n",
    "#     calibrated.fit(X_train, y_train)\n",
    "#     # predict probabilities\n",
    "#     return calibrated.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "# # uncalibrated predictions\n",
    "# for model in clf.keys():\n",
    "#     unc = uncalibrated(X_train, X_test, y_train, model)\n",
    "#     fop_uncalibrated, mpv_uncalibrated = calibration_curve(y_test, unc, n_bins=10, normalize=True)\n",
    "#     # plot model reliabilities\n",
    "#     plt.plot(mpv_uncalibrated, fop_uncalibrated, marker='.', label=model.upper())\n",
    "    \n",
    "# # calibrated predictions\n",
    "# cal = calibrated(X_train, X_test, y_train)\n",
    "# fop_calibrated, mpv_calibrated = calibration_curve(y_test, cal, n_bins=10)\n",
    "# plt.plot(mpv_calibrated, fop_calibrated, marker='.', label='Calibrated SVM')\n",
    "\n",
    "# # plot perfectly calibrated\n",
    "# plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "\n",
    "# # plt.plot(mpv_calibrated, fop_calibrated, marker='.')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Precision Recall Curve vs ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.set_xlim([-0.05,1.05])\n",
    "ax1.set_ylim([-0.05,1.05])\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title('PR Curve')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.set_xlim([-0.05,1.05])\n",
    "ax2.set_ylim([-0.05,1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curve')\n",
    "\n",
    "for model, color in zip(clf.keys(),'bgr'):\n",
    "    \n",
    "    try:\n",
    "        pred_prob = clf[model].predict_proba(X_test)[:,1]\n",
    "    \n",
    "    except AttributeError:\n",
    "        pred_prob = clf[model].decision_function(X_test)[:,1]\n",
    "\n",
    "    p,r,_ = precision_recall_curve(y_test, pred_prob)\n",
    "    tpr,fpr,_ = roc_curve(y_test, pred_prob)\n",
    "    \n",
    "    ax1.plot(r, p, c=color, label=model.upper())\n",
    "    ax2.plot(tpr, fpr, c=color, label=model.upper())\n",
    "ax1.legend(loc='lower left')    \n",
    "ax2.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "for model in clf.keys():\n",
    "    print('---Classification Report ('+model.upper()+')---')\n",
    "    print(classification_report(y_test, y_pred[model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
