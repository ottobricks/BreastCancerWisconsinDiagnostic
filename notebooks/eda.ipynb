{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.eda import BCW_Explorer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda = BCW_Explorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Analysis\n",
    "\n",
    "## 2.1 Overview of Variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Attributes:\\n\\t{0}\\n'.format(\n",
    "        ', '.join(str(x) for x in eda.data.columns.get_level_values('attributes').unique() if x)\n",
    "    ),\n",
    "    '\\n... where each attribute is summarized by:\\n\\t{0}'.format(\n",
    "        ', '.join(str(x) for x in eda.data.columns.get_level_values('summaries').unique() if x != 'label')\n",
    "    ),\n",
    "    '\\n\\n... for a total of 30 features'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "At first, we should assume that there is correlation between some of the features given the nature of the attributes which they summarize.\n",
    "* radius yields (although the attribute is the mean of distances from center to points on the perimeter)\n",
    "    * perimiter, area, smoothness (local variation in radius lengths) and compactness (perimeter^2 / area - 1.0);\n",
    "\n",
    "We explore whether they significantly correlate in the section 2.3\n",
    "<br />\n",
    "\n",
    "\\> **What is the attribute Fractal Dimension?**<br />\n",
    "&emsp;Fractal dimension is a measure of how regular the contour of a shape is.<br />\n",
    "It is approximated by measuring the downward slope of the log of observed perimeter plotted agains the log of a \"ruler\" size.<br />\n",
    "Hence, a higher value corresponds to a less regular contour, which in turn means a higher probability of malignancy (Wolberg et al. 1994)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2 Research Questions (RQs)\n",
    "\n",
    "1. What are the most significant features, and how to measure significance?\n",
    "2. Which classifiers are most suitable for the task, and how to decide?\n",
    "3. How much better (or worse) are classifiers induced from reduced dimensionality?\n",
    "4. What performance measure should be favored during hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.3 Class Imbalance, Feature Distribution and Selection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_classdistribution(in_df: pd.DataFrame, summary='mean', ax=None):\n",
    "    # scales the data\n",
    "    df = in_df.apply(preprocessing.scale, axis=0).copy()\n",
    "    # df['label'] = df['label'].map({1: 'Malignant', 0: 'Benign'})\n",
    "\n",
    "    # melts columns' (name, value) pair into two columns\n",
    "    df =\\\n",
    "        pd.melt(\n",
    "            pd.concat(\n",
    "                [\n",
    "                    df['label'],\n",
    "                    df.xs(summary, level='summaries', axis=1)\n",
    "                ],\n",
    "                axis=1\n",
    "            ),\n",
    "            id_vars='label',\n",
    "            var_name='features',\n",
    "            value_name='value'\n",
    "        )\n",
    "\n",
    "    # plots comparative distribution of classes for each 'mean' feature\n",
    "    return\\\n",
    "        sns.violinplot(\n",
    "            ax=ax,\n",
    "            data=df,\n",
    "            y=\"value\",\n",
    "            x=\"features\",\n",
    "            hue=\"label\",\n",
    "            inner=\"quartile\",\n",
    "            split=True,\n",
    "        )\n",
    "\n",
    "from scipy.stats import boxcox, skew\n",
    "\n",
    "\n",
    "def transform_skew(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Transforms data with Boxcox transformation\n",
    "    Keeps the transformation only if it is less skewed than the original\n",
    "    '''\n",
    "    skew_transformed = pd.DataFrame()\n",
    "\n",
    "    for feat in df.drop('label', axis=1):\n",
    "\n",
    "        data = df.drop('label', axis=1)[feat].values\n",
    "        posdata = data[data > 0]\n",
    "        # posdata = eda.data[eda.data[feat]>0][feat]\n",
    "\n",
    "        x, lmbda = boxcox(posdata, lmbda=None)\n",
    "        \n",
    "        transform = np.empty_like(data)\n",
    "        transform[data > 0] = x\n",
    "        transform[data == 0] = -1/lmbda\n",
    "\n",
    "        if abs(skew(transform)) < abs(skew(data)):\n",
    "            skew_transformed[feat] = transform\n",
    "\n",
    "        else:\n",
    "            skew_transformed[feat] = data\n",
    "    \n",
    "    skew_transformed.columns =\\\n",
    "        pd.MultiIndex.from_tuples(\n",
    "            list(skew_transformed.columns),\n",
    "            names=('summaries', 'attributes')\n",
    "        )\n",
    "\n",
    "    return skew_transformed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.set_title('Balance of Classes', fontsize=14)\n",
    "\n",
    "eda.data['label']\\\n",
    "    .map({1: 'Malignant', 0: 'Benign'})\\\n",
    "    .value_counts()\\\n",
    "    .plot(\n",
    "        ax=ax,\n",
    "        kind='pie',\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=270,\n",
    "        fontsize=14,\n",
    "        label=''\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Imbalanced Classes\n",
    "&emsp;There are almost twice as many benign observations than malignant, which can become an issue while inducing machine learning models. Thus, we must employ a sampling technique.<br />\n",
    "<br />\n",
    "\n",
    "<center><img style=\"margin-bottom:5mm\" src=\"https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png\"><center/>\n",
    "<center>Figure 1. Approaches to resampling<center/>\n",
    "<center style=\"margin-bottom:5mm\">Source. https://tinyurl.com/yy2qtbe2<center/>\n",
    "\n",
    "<p>&emsp;However, before applying any resampling method to our data, we must ensure that it meets some assumptions that sampling methods require; thus, we investigate the distribution of our features and whether there are any redundant candidates for removal. From both tasks, the former is comprised of statistical analysis and data visualization, while the latter of feature selection techniques (e.g. correlation analysis and recursive feature elimination (RFE))<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "\n",
    "ax.set_title('Comparative Distribution of Features (standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(eda.data, ax=ax, summary='mean')\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "leg = ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skewness of features:')\n",
    "eda.data.skew().sort_values(ascending=False).sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies boxcox transformation and decides whether to use it against the original variable\n",
    "transformed = transform_skew(eda.data)\n",
    "transformed['label'] = eda.data['label']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "\n",
    "ax.set_title('Comparative Distribution of Features (skewed transformed, standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(transformed)\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skewness after Boxcox transform:')\n",
    "transformed.skew().sort_values(ascending=False).sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_correlation(data: pd.DataFrame, threshold=0.5) -> pd.DataFrame:\n",
    "    try:\n",
    "        tmp = data.drop('label', axis=1).copy()\n",
    "    \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    tmp.columns = tmp.columns.to_flat_index()\n",
    "    dataCorr = tmp.corr(method='pearson')\n",
    "\n",
    "    dataCorr = dataCorr.mask(np.tril(np.ones(dataCorr.shape)).astype(np.bool))\n",
    "    dataCorr = dataCorr[abs(dataCorr) >= threshold].stack()\n",
    "\n",
    "    dataCorr = dataCorr.reset_index(level=0)\n",
    "    dataCorr.index = pd.MultiIndex.from_tuples(dataCorr.index, names=('summaries', 'attributes'))\n",
    "    \n",
    "    return dataCorr.sort_index(level='attributes').sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(17, 12))\n",
    "ax.set_title('Correlation Matrix', fontsize=14)\n",
    "\n",
    "corr = eda.data.drop('label', axis=1).corr()\n",
    "\n",
    "# generates an upper diagonal mask\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    ax=ax,\n",
    "    vmax=1,\n",
    "    cmap=cmap,\n",
    "    mask=mask,\n",
    "    annot=False,\n",
    "    square=False,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Feature Selection Stategy**<br />\n",
    "&emsp;Before applying an automated search for the optimal dimensionality for each classifer, we investigate features we consider relevant for the sake of comparison. There are a few features (variables) that correlate with many others, which could lead to using them to represent all others. We must how these \"subfeatures\" correlated to one another. At the end of this process, we will be able to answer RQ1.\n",
    "* RQ1 - What are the most significant features, and how to measure significance?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selects features that have highest number of correlations above the given threshold\n",
    "# above_threshold = corr.apply(lambda x: x > .9).sum()\n",
    "# most_correlations = corr[above_threshold.apply(lambda x: x == above_threshold.max())]\n",
    "# most_correlations[most_correlations.apply(lambda x: x > 0.9)].dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Can we find clusters of correlation to select one representative feature?**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.index.values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# X = get_unique_correlation(eda.data)\n",
    "\n",
    "def correlation_clustering(corr: np.ndarray, labels: np.ndarray, threshold=0.5, ax=matplotlib.axes._subplots) -> (dict, np.ndarray):\n",
    "    '''\n",
    "    '''\n",
    "    dissimilarity = 1 - np.abs(corr)\n",
    "    hierarchy = linkage(squareform(dissimilarity), method='single', metric='correlation')\n",
    "    \n",
    "    return\\\n",
    "        dendrogram(hierarchy, color_threshold=None, labels=labels, ax=ax, distance_sort=True, leaf_font_size=12, orientation='right'),\\\n",
    "        fcluster(hierarchy, threshold, criterion='distance')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "ax.set_title('Correlation Clustering', fontsize=14)\n",
    "\n",
    "d, labels = correlation_clustering(corr.values, corr.index.values, ax=ax)\n",
    "\n",
    "# plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labels, corr.index.values, name='Feature Cluster').sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# xs = eda.data.loc[:,[('mean_max3', 'area')]]\n",
    "# ys = eda.data.loc[:,[('mean', 'texture')]]\n",
    "# zs = eda.data.loc[:,[('mean_max3', 'smoothness')]]\n",
    "# ax.scatter(\n",
    "#     xs, ys, zs,\n",
    "#     edgecolors='w',\n",
    "#     cmap = 'rainbow',\n",
    "#     s=50, alpha=0.6,\n",
    "#     c=eda.data.loc[:, ('label')],\n",
    "# )\n",
    "\n",
    "# ax.set_xlabel('Area worst_3 mean')\n",
    "# ax.set_ylabel('Texture mean')\n",
    "# ax.set_zlabel('Smoothness worst_3 mean')\n",
    "\n",
    "# plt.show()\n",
    "data = eda.data.loc[:,[('mean_max3', 'radius'), ('mean', 'texture'), ('mean_max3', 'smoothness'), ('label', '')]].droplevel(0, axis=1).rename(columns={'': 'label'})\n",
    "fig =\\\n",
    "    px.scatter_3d(data, x='radius', y='texture', z='smoothness', color='label')\n",
    "\n",
    "fig.show()\n",
    "  \n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "data = transformed.loc[:,[('mean_max3', 'radius'), ('mean', 'texture'), ('mean_max3', 'smoothness'), ('label', '')]].droplevel(0, axis=1).rename(columns={'': 'label'})\n",
    "\n",
    "fig =\\\n",
    "    px.scatter_3d(data, x='radius', y='texture', z='smoothness', color='label')\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}