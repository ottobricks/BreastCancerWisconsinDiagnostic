{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ottok92/Dev/elogroup2/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.eda import BCW_Explorer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "eda = BCW_Explorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "eda.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Analysis\n",
    "\n",
    "## 2.1 Overview of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    'Attributes:\\n\\t{0}\\n'.format(\n",
    "        ', '.join(str(x) for x in eda.data.columns.get_level_values('attributes').unique() if x)\n",
    "    ),\n",
    "    '\\n... where each attribute is summarized by:\\n\\t{0}'.format(\n",
    "        ', '.join(str(x) for x in eda.data.columns.get_level_values('summaries').unique() if x != 'label')\n",
    "    ),\n",
    "    '\\n\\n... for a total of 30 features'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "At first, we should assume that there is correlation between some of the features given the nature of the attributes which they summarize.\n",
    "* radius yields (although the attribute is the mean of distances from center to points on the perimeter)\n",
    "    * perimiter, area, smoothness (local variation in radius lengths) and compactness (perimeter^2 / area - 1.0);\n",
    "\n",
    "We explore whether they significantly correlate in the section 2.3\n",
    "<br />\n",
    "\n",
    "\\> **What is the attribute Fractal Dimension?**<br />\n",
    "&emsp;Fractal dimension is a measure of how regular the contour of a shape is.<br />\n",
    "It is approximated by measuring the downward slope of the log of observed perimeter plotted agains the log of a \"ruler\" size.<br />\n",
    "Hence, a higher value corresponds to a less regular contour, which in turn means a higher probability of malignancy (Wolberg et al. 1994)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Research Questions (RQs)\n",
    "\n",
    "1. What are the most significant features, and how to measure significance?\n",
    "2. Which classifiers are most suitable for the task, and how to decide?\n",
    "3. How much better (or worse) are classifiers induced from reduced dimensionality?\n",
    "4. What performance measure should be favored during hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Class Imbalance, Feature Distribution and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.set_title('Balance of Classes', fontsize=14)\n",
    "\n",
    "eda.data['label']\\\n",
    "    .map({1: 'Malignant', 0: 'Benign'})\\\n",
    "    .value_counts()\\\n",
    "    .plot(\n",
    "        ax=ax,\n",
    "        kind='pie',\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=270,\n",
    "        fontsize=14,\n",
    "        label=''\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Imbalanced Classes\n",
    "&emsp;There are almost twice as many benign observations than malignant, which can become an issue while inducing machine learning models. We can either employ a sampling technique or adjust our assessment of models to account for such imbalance. The former entails taking advantage of techniques such as SMOTE to generate new samples that follow the distribution of a give class, the latter means going beyond accuracy measures and throroughly assessing the assumptions and aims of other measures such as Confusion Matrix and Reliability Diagrams.<br />\n",
    "<br />\n",
    "\n",
    "<!-- <center><img style=\"margin-bottom:5mm\" src=\"https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png\"><center/>\n",
    "<center>Figure 1. Approaches to resampling<center/>\n",
    "<center style=\"margin-bottom:5mm\">Source. https://tinyurl.com/yy2qtbe2<center/>\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "X = eda.data.drop(('label', ''), axis=1).values\n",
    "y = eda.data.droplevel(1, axis=1)['label'].values\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_sample(X, y)\n",
    "\n",
    "plt.subplot(111)\n",
    "plot_2d_space(X, y, 'No over-sampling')\n",
    "\n",
    "plt.subplot(111)\n",
    "plot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')\n",
    "\n",
    "del X, y, X_sm, y_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&emsp;However, before applying any resampling method to our data, we must ensure that it meets some assumptions that sampling methods require; thus, we investigate the distribution of our features and whether there are any redundant candidates for removal. From both tasks, the former is comprised of statistical analysis and data visualization, while the latter of feature selection techniques (e.g. correlation analysis and recursive feature elimination (RFE))<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox, skew\n",
    "\n",
    "def plot_compare_classdistribution(in_df: pd.DataFrame, summary=None, ax=None, feats=None):\n",
    "    '''\n",
    "    '''\n",
    "    try:\n",
    "        df = in_df[feats].apply(preprocessing.scale, axis=0).copy()\n",
    "    \n",
    "    except KeyError:\n",
    "        df = in_df.apply(preprocessing.scale, axis=0).copy()\n",
    "\n",
    "    try:\n",
    "        data = df.xs(summary, level='summaries', axis=1)\n",
    "    \n",
    "    except KeyError:\n",
    "        data = df\n",
    "\n",
    "    # melts columns' (name, value) pair into two columns\n",
    "    df =\\\n",
    "        pd.melt(\n",
    "            pd.concat(\n",
    "                [\n",
    "                    eda.data.droplevel(1, axis=1)['label'],\n",
    "                    data\n",
    "                ],\n",
    "                axis=1\n",
    "            ),\n",
    "            id_vars='label',\n",
    "            var_name='features',\n",
    "            value_name='value'\n",
    "        )\n",
    "\n",
    "    # plots comparative distribution of classes for each 'mean' feature\n",
    "    return\\\n",
    "        sns.violinplot(\n",
    "            ax=ax,\n",
    "            data=df,\n",
    "            y=\"value\",\n",
    "            x=\"features\",\n",
    "            hue=\"label\",\n",
    "            inner=\"quartile\",\n",
    "            split=True,\n",
    "        )\n",
    "\n",
    "\n",
    "def transform_skew(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Transforms data with Boxcox transformation\n",
    "    Keeps the transformation only if it is less skewed than the original\n",
    "    '''\n",
    "    skew_transformed = pd.DataFrame()\n",
    "\n",
    "    for feat in df.drop('label', axis=1):\n",
    "\n",
    "        data = df.drop('label', axis=1)[feat].values\n",
    "        posdata = data[data > 0]\n",
    "        # posdata = eda.data[eda.data[feat]>0][feat]\n",
    "\n",
    "        x, lmbda = boxcox(posdata, lmbda=None)\n",
    "        \n",
    "        transform = np.empty_like(data)\n",
    "        transform[data > 0] = x\n",
    "        transform[data == 0] = -1/lmbda\n",
    "\n",
    "        if abs(skew(transform)) < abs(skew(data)):\n",
    "            skew_transformed[feat] = transform\n",
    "\n",
    "        else:\n",
    "            skew_transformed[feat] = data\n",
    "    \n",
    "    skew_transformed.columns =\\\n",
    "        pd.MultiIndex.from_tuples(\n",
    "            list(skew_transformed.columns),\n",
    "            names=('summaries', 'attributes')\n",
    "        )\n",
    "\n",
    "    return skew_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "\n",
    "ax.set_title('Comparative Distribution of Features (standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(eda.data, ax=ax, summary='mean')\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "leg = ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "print('Skewness of features:')\n",
    "eda.data.skew().sort_values(ascending=False).sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# applies boxcox transformation and decides whether to use it against the original variable\n",
    "transformed = transform_skew(eda.data)\n",
    "transformed['label'] = eda.data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "\n",
    "ax.set_title('Comparative Distribution of Features (skewed transformed, standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(transformed, summary='mean')\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "print('Skewness after Boxcox transform:')\n",
    "transformed.skew().sort_values(ascending=False).sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_unique_correlation(data: pd.DataFrame, threshold=0.5) -> pd.DataFrame:\n",
    "    try:\n",
    "        tmp = data.drop('label', axis=1).copy()\n",
    "    \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    tmp.columns = tmp.columns.to_flat_index()\n",
    "    dataCorr = tmp.corr(method='pearson')\n",
    "\n",
    "    dataCorr = dataCorr.mask(np.tril(np.ones(dataCorr.shape)).astype(np.bool))\n",
    "    dataCorr = dataCorr[abs(dataCorr) >= threshold].stack()\n",
    "\n",
    "    dataCorr = dataCorr.reset_index(level=0)\n",
    "    dataCorr.index = pd.MultiIndex.from_tuples(dataCorr.index, names=('summaries', 'attributes'))\n",
    "    \n",
    "    return dataCorr.sort_index(level='attributes').sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(17, 12))\n",
    "ax.set_title('Correlation Matrix', fontsize=14)\n",
    "\n",
    "corr = eda.data.drop('label', axis=1).corr()\n",
    "\n",
    "# generates an upper diagonal mask\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    ax=ax,\n",
    "    vmax=1,\n",
    "    cmap=cmap,\n",
    "    mask=mask,\n",
    "    annot=False,\n",
    "    square=False,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection Stategy**<br />\n",
    "&emsp;Before applying an automated search for the optimal dimensionality for each classifer, we investigate features we consider relevant for the sake of comparison. There are a few features (variables) that correlate with many others, which could lead to using them to represent all others. We must how these \"subfeatures\" correlated to one another. At the end of this process, we will be able to answer RQ1.\n",
    "* RQ1 - What are the most significant features, and how to measure significance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation clustering** is the task of ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def correlation_clustering(corr: np.ndarray, labels: np.ndarray, threshold=0.5, ax=plt.axes, maxclust=None) -> (dict or np.ndarray):\n",
    "    '''\n",
    "    '''\n",
    "    dissimilarity = 1 - np.abs(corr)\n",
    "    hierarchy = linkage(squareform(dissimilarity), method='ward', metric='distance')\n",
    "    \n",
    "    return\\\n",
    "        (\n",
    "            fcluster(hierarchy, maxclust, criterion='maxclust') if maxclust\n",
    "            else\n",
    "            dendrogram(\n",
    "                hierarchy, color_threshold=None,\n",
    "                labels=labels, ax=ax,\n",
    "                distance_sort='ascending',\n",
    "                leaf_font_size=12,\n",
    "                orientation='right',\n",
    "            )\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "ax.set_title('Correlation Clustering', fontsize=14)\n",
    "\n",
    "d = correlation_clustering(corr.values, corr.index.values, ax=ax)\n",
    "\n",
    "# plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# groups features into their respective clusters\n",
    "cluster = correlation_clustering(corr.values, corr.index.values, maxclust=3)\n",
    "cluster =\\\n",
    "    pd.Series(cluster, corr.index.values, name='cluster')\\\n",
    "        .reset_index().groupby('cluster')['index'].apply(np.array)\\\n",
    "        .to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "eda.data[cluster[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "@interact\n",
    "def viz_cluster_separation(\n",
    "    feature1=[pair[0] +'_'+ pair[1] for pair in cluster[1]],\n",
    "    feature2=[pair[0] +'_'+ pair[1] for pair in cluster[2]],\n",
    "    feature3=[pair[0] +'_'+ pair[1] for pair in cluster[3]]\n",
    "):\n",
    "    '''\n",
    "    '''\n",
    "    labels = eda.data[('label', '')]\n",
    "    feature1 = tuple(feature1.split('_', 1))\n",
    "    feature2 = tuple(feature2.split('_', 1))\n",
    "    feature3 = tuple(feature3.split('_', 1))\n",
    "\n",
    "    fig =\\\n",
    "        make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            specs=[\n",
    "                [{'is_3d': True}, {'is_3d': True}]\n",
    "            ],\n",
    "            print_grid=False,\n",
    "            subplot_titles=('Original features', 'Standardized features')\n",
    "        )\n",
    "\n",
    "    x = eda.data[feature1]\n",
    "    y = eda.data[feature2]\n",
    "    z = eda.data[feature3]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode='markers',\n",
    "            showlegend=False,\n",
    "            \n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=labels,\n",
    "                colorscale=['#CC8963', '#5875A4'],\n",
    "                opacity=1.0\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    x = transformed[feature1]\n",
    "    y = transformed[feature2]\n",
    "    z = transformed[feature3]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode='markers',\n",
    "            showlegend=False,\n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=labels,\n",
    "                colorscale=['#CC8963', '#5875A4'],\n",
    "                opacity=1.0\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        margin=dict(r=10, l=10, b=10, t=10),\n",
    "        title_text='Class separation',\n",
    "        scene = dict(\n",
    "            xaxis_title='Mean Texture',\n",
    "            yaxis_title='Worst Area',\n",
    "            zaxis_title='Worst Smoothness'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selects** the feature that has highest correlation to target variable ('label') within each cluster (RQ1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "target = eda.data.corr()[('label', '')]\n",
    "selected_feats = [target[cluster[k]].idxmax() for k in cluster.keys()]\n",
    "selected_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# ax[0].set_title('Distribution of selected features (standardized)', fontsize=14)\n",
    "# ax[0].spines['top'].set_visible(False)\n",
    "# ax[0].spines['bottom'].set_visible(False)\n",
    "# ax[0].spines['right'].set_visible(False)\n",
    "# ax[0].spines['left'].set_visible(False)\n",
    "\n",
    "# plot_compare_classdistribution(eda.data, ax=ax[0], feats=selected_feats)\n",
    "\n",
    "# ax[0].tick_params(axis='x', labelrotation=30)\n",
    "# ax[0].legend_.remove()\n",
    "# leg = ax[0].legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "ax.set_title('Distribution of selected features (skewed tranformed and standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(transformed, ax=ax, feats=selected_feats)\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "leg = ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "**Validate** clusters with Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVR\n",
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "# estimator = SVR(kernel=\"linear\")\n",
    "# selector = RFE(estimator, 3, step=1)\n",
    "\n",
    "# selector =\\\n",
    "#     selector.fit(\n",
    "#         transformed.drop(('label', ''), axis=1).values,\n",
    "#         transformed[('label', '')].values\n",
    "#     )\n",
    "\n",
    "# transformed.drop(('label', ''), axis=1).columns.to_flat_index()[selector.support_].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Selection and Induction (RQ2)\n",
    "Which classifiers are most suitable for the task, and how to decide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splits data and labels\n",
    "X = eda.data[selected_feats].values\n",
    "y = eda.data.droplevel(1, axis=1)['label'].values\n",
    "\n",
    "Xstd = transformed[selected_feats].values\n",
    "ystd = transformed.droplevel(1, axis=1)['label'].values\n",
    "\n",
    "# splits train and test\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "Xstd_train, Xstd_test, ystd_train, ystd_test =\\\n",
    "    train_test_split(\n",
    "        Xstd,\n",
    "        ystd,\n",
    "        test_size=.2,\n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "class ClfSwitcher(BaseEstimator):\n",
    "    '''\n",
    "    Boilerplate class to support hyperparameter tunning for multiple classifiers\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        estimator = SGDClassifier(),\n",
    "    ): \n",
    "        self.estimator = estimator\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.estimator.score(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# from sklearn import feature_selection\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# class PipelineRFE(Pipeline):\n",
    "\n",
    "#     def fit(self, X, y=None, **fit_params):\n",
    "#         super(PipelineRFE, self).fit(X, y, **fit_params)\n",
    "#         self.feature_importances_ = self.steps[-1][-1].feature_importances_\n",
    "#         return self\n",
    "\n",
    "# pipe = PipelineRFE(\n",
    "#     [\n",
    "#         ('std_scaler', preprocessing.StandardScaler()),\n",
    "#         (\"ET\", ExtraTreesRegressor(random_state=42, n_estimators=250, shuffle=True))\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Sets RNG seed to reproduce results (your results should match mine)\n",
    "# _ = StratifiedKFold(random_state=42)\n",
    "\n",
    "# feature_selector_cv = feature_selection.RFECV(pipe, cv=10, step=1, scoring=\"neg_mean_squared_error\")\n",
    "# feature_selector_cv.fit(X_train, y_train)\n",
    "# eda.data.drop(('label', ''), axis=1).columns.to_flat_index()[feature_selector_cv.support_].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Reduced vs Full Dimensionality (RQ3)\n",
    "How much better (or worse) are classifiers induced from reduced dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Assessing Results (RQ4)\n",
    "What performance measure should be favored during hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Reliability Diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Precision Recall Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
