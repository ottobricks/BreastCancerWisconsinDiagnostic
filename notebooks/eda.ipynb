{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ottok92/Dev/elogroup2/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.eda import BCW_Explorer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda = BCW_Explorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Analysis\n",
    "\n",
    "## 2.1 Overview of Variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Attributes:\\n\\t{0}\\n'.format(\n",
    "        ', '.join(str(x) for x in eda.data.columns.get_level_values('attributes').unique() if x)\n",
    "    ),\n",
    "    '\\n... where each attribute is summarized by:\\n\\t{0}'.format(\n",
    "        ', '.join(str(x) for x in eda.data.columns.get_level_values('summaries').unique() if x != 'label')\n",
    "    ),\n",
    "    '\\n\\n... for a total of 30 features'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "At first, we should assume that there is correlation between some of the features given the nature of the attributes which they summarize.\n",
    "* radius yields (although the attribute is the mean of distances from center to points on the perimeter)\n",
    "    * perimiter, area, smoothness (local variation in radius lengths) and compactness (perimeter^2 / area - 1.0);\n",
    "\n",
    "We explore whether they significantly correlate in the section 2.3\n",
    "<br />\n",
    "\n",
    "\\> **What is the attribute Fractal Dimension?**<br />\n",
    "&emsp;Fractal dimension is a measure of how regular the contour of a shape is.<br />\n",
    "It is approximated by measuring the downward slope of the log of observed perimeter plotted agains the log of a \"ruler\" size.<br />\n",
    "Hence, a higher value corresponds to a less regular contour, which in turn means a higher probability of malignancy (Wolberg et al. 1994)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2 Research Questions (RQs)\n",
    "\n",
    "1. What are the most significant features, and how to measure significance?\n",
    "2. Which classifiers are most suitable for the task, and how to decide?\n",
    "3. How much better (or worse) are classifiers induced from reduced dimensionality?\n",
    "4. What performance measure should be favored during hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.3 Class Imbalance, Feature Distribution and Selection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_classdistribution(in_df: pd.DataFrame, summary='mean', ax=None):\n",
    "    # scales the data\n",
    "    df = in_df.apply(preprocessing.scale, axis=0).copy()\n",
    "    # df['label'] = df['label'].map({1: 'Malignant', 0: 'Benign'})\n",
    "\n",
    "    # melts columns' (name, value) pair into two columns\n",
    "    df =\\\n",
    "        pd.melt(\n",
    "            pd.concat(\n",
    "                [\n",
    "                    df['label'],\n",
    "                    df.xs(summary, level='summaries', axis=1)\n",
    "                ],\n",
    "                axis=1\n",
    "            ),\n",
    "            id_vars='label',\n",
    "            var_name='features',\n",
    "            value_name='value'\n",
    "        )\n",
    "\n",
    "    # plots comparative distribution of classes for each 'mean' feature\n",
    "    return\\\n",
    "        sns.violinplot(\n",
    "            ax=ax,\n",
    "            data=df,\n",
    "            y=\"value\",\n",
    "            x=\"features\",\n",
    "            hue=\"label\",\n",
    "            inner=\"quartile\",\n",
    "            split=True,\n",
    "        )\n",
    "\n",
    "from scipy.stats import boxcox, skew\n",
    "\n",
    "\n",
    "def transform_skew(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Transforms data with Boxcox transformation\n",
    "    Keeps the transformation only if it is less skewed than the original\n",
    "    '''\n",
    "    skew_transformed = pd.DataFrame()\n",
    "\n",
    "    for feat in df.drop('label', axis=1):\n",
    "\n",
    "        data = df.drop('label', axis=1)[feat].values\n",
    "        posdata = data[data > 0]\n",
    "        # posdata = eda.data[eda.data[feat]>0][feat]\n",
    "\n",
    "        x, lmbda = boxcox(posdata, lmbda=None)\n",
    "        \n",
    "        transform = np.empty_like(data)\n",
    "        transform[data > 0] = x\n",
    "        transform[data == 0] = -1/lmbda\n",
    "\n",
    "        if abs(skew(transform)) < abs(skew(data)):\n",
    "            skew_transformed[feat] = transform\n",
    "\n",
    "        else:\n",
    "            skew_transformed[feat] = data\n",
    "    \n",
    "    skew_transformed.columns =\\\n",
    "        pd.MultiIndex.from_tuples(\n",
    "            list(skew_transformed.columns),\n",
    "            names=('summaries', 'attributes')\n",
    "        )\n",
    "\n",
    "    return skew_transformed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.set_title('Balance of Classes', fontsize=14)\n",
    "\n",
    "eda.data['label']\\\n",
    "    .map({1: 'Malignant', 0: 'Benign'})\\\n",
    "    .value_counts()\\\n",
    "    .plot(\n",
    "        ax=ax,\n",
    "        kind='pie',\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=270,\n",
    "        fontsize=14,\n",
    "        label=''\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Imbalanced Classes\n",
    "&emsp;There are almost twice as many benign observations than malignant, which can become an issue while inducing machine learning models. Thus, we must employ a sampling technique.<br />\n",
    "<br />\n",
    "\n",
    "<center><img style=\"margin-bottom:5mm\" src=\"https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png\"><center/>\n",
    "<center>Figure 1. Approaches to resampling<center/>\n",
    "<center style=\"margin-bottom:5mm\">Source. https://tinyurl.com/yy2qtbe2<center/>\n",
    "\n",
    "<p>&emsp;However, before applying any resampling method to our data, we must ensure that it meets some assumptions that sampling methods require; thus, we investigate the distribution of our features and whether there are any redundant candidates for removal. From both tasks, the former is comprised of statistical analysis and data visualization, while the latter of feature selection techniques (e.g. correlation analysis and recursive feature elimination (RFE))<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "\n",
    "ax.set_title('Comparative Distribution of Features (standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(eda.data, ax=ax, summary='mean')\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "leg = ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skewness of features:')\n",
    "eda.data.skew().sort_values(ascending=False).sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies boxcox transformation and decides whether to use it against the original variable\n",
    "transformed = transform_skew(eda.data)\n",
    "transformed['label'] = eda.data['label']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "\n",
    "ax.set_title('Comparative Distribution of Features (skewed transformed, standardized)', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plot_compare_classdistribution(transformed)\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=30)\n",
    "ax.legend_.remove()\n",
    "ax.legend(['Quartiles', 'Mean'], loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skewness after Boxcox transform:')\n",
    "transformed.skew().sort_values(ascending=False).sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_correlation(data: pd.DataFrame, threshold=0.5) -> pd.DataFrame:\n",
    "    try:\n",
    "        tmp = data.drop('label', axis=1).copy()\n",
    "    \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    tmp.columns = tmp.columns.to_flat_index()\n",
    "    dataCorr = tmp.corr(method='pearson')\n",
    "\n",
    "    dataCorr = dataCorr.mask(np.tril(np.ones(dataCorr.shape)).astype(np.bool))\n",
    "    dataCorr = dataCorr[abs(dataCorr) >= threshold].stack()\n",
    "\n",
    "    dataCorr = dataCorr.reset_index(level=0)\n",
    "    dataCorr.index = pd.MultiIndex.from_tuples(dataCorr.index, names=('summaries', 'attributes'))\n",
    "    \n",
    "    return dataCorr.sort_index(level='attributes').sort_index(level='summaries', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(17, 12))\n",
    "ax.set_title('Correlation Matrix', fontsize=14)\n",
    "\n",
    "corr = eda.data.drop('label', axis=1).corr()\n",
    "\n",
    "# generates an upper diagonal mask\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    ax=ax,\n",
    "    vmax=1,\n",
    "    cmap=cmap,\n",
    "    mask=mask,\n",
    "    annot=False,\n",
    "    square=False,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Feature Selection Stategy**<br />\n",
    "&emsp;Before applying an automated search for the optimal dimensionality for each classifer, we investigate features we consider relevant for the sake of comparison. There are a few features (variables) that correlate with many others, which could lead to using them to represent all others. We must how these \"subfeatures\" correlated to one another. At the end of this process, we will be able to answer RQ1.\n",
    "* RQ1 - What are the most significant features, and how to measure significance?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selects features that have highest number of correlations above the given threshold\n",
    "# above_threshold = corr.apply(lambda x: x > .9).sum()\n",
    "# most_correlations = corr[above_threshold.apply(lambda x: x == above_threshold.max())]\n",
    "# most_correlations[most_correlations.apply(lambda x: x > 0.9)].dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Correlation clustering** is the task of ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_clustering(corr: np.ndarray, labels: np.ndarray, threshold=0.5, ax=plt.axes, maxclust=None) -> (dict or np.ndarray):\n",
    "    '''\n",
    "    '''\n",
    "    dissimilarity = 1 - np.abs(corr)\n",
    "    hierarchy = linkage(squareform(dissimilarity), method='ward', metric='distance')\n",
    "    \n",
    "    return\\\n",
    "        (\n",
    "            fcluster(hierarchy, maxclust, criterion='maxclust') if maxclust\n",
    "            else\n",
    "            dendrogram(\n",
    "                hierarchy, color_threshold=None,\n",
    "                labels=labels, ax=ax,\n",
    "                distance_sort='ascending',\n",
    "                leaf_font_size=12,\n",
    "                orientation='right',\n",
    "            )\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "ax.set_title('Correlation Clustering', fontsize=14)\n",
    "\n",
    "d = correlation_clustering(corr.values, corr.index.values, ax=ax)\n",
    "\n",
    "# plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups features into their respective clusters\n",
    "cluster = correlation_clustering(corr.values, corr.index.values, maxclust=3)\n",
    "cluster =\\\n",
    "    pd.Series(cluster, corr.index.values, name='cluster')\\\n",
    "        .reset_index().groupby('cluster')['index'].apply(np.array)\\\n",
    "        .to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.data[cluster[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "@interact\n",
    "def viz_cluster_separation(\n",
    "    feature1=[pair[0] +'_'+ pair[1] for pair in cluster[1]],\n",
    "    feature2=[pair[0] +'_'+ pair[1] for pair in cluster[2]],\n",
    "    feature3=[pair[0] +'_'+ pair[1] for pair in cluster[3]]\n",
    "):\n",
    "    '''\n",
    "    '''\n",
    "    labels = eda.data[('label', '')]\n",
    "    feature1 = tuple(feature1.split('_', 1))\n",
    "    feature2 = tuple(feature2.split('_', 1))\n",
    "    feature3 = tuple(feature3.split('_', 1))\n",
    "\n",
    "    fig =\\\n",
    "        make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            specs=[\n",
    "                [{'is_3d': True}, {'is_3d': True}]\n",
    "            ],\n",
    "            print_grid=False,\n",
    "            subplot_titles=('Original features', 'Standardized features')\n",
    "        )\n",
    "\n",
    "    x = eda.data[feature1]\n",
    "    y = eda.data[feature2]\n",
    "    z = eda.data[feature3]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode='markers',\n",
    "            showlegend=False,\n",
    "            \n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=labels,\n",
    "                colorscale=['#CC8963', '#5875A4'],\n",
    "                opacity=1.0\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    x = transformed[feature1]\n",
    "    y = transformed[feature2]\n",
    "    z = transformed[feature3]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode='markers',\n",
    "            showlegend=False,\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=labels,\n",
    "                colorscale=['#CC8963', '#5875A4'],\n",
    "                opacity=1.0\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        margin=dict(r=10, l=10, b=10, t=10),\n",
    "        title_text='Class separation',\n",
    "        scene = dict(\n",
    "            xaxis_title='Mean Texture',\n",
    "            yaxis_title='Worst Area',\n",
    "            zaxis_title='Worst Smoothness'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "**Validate** clusters with Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "std_X_train, std_X_test, std_y_train, std_y_test =\\\n",
    "    train_test_split(\n",
    "        transformed.drop(('label', ''), axis=1).values,\n",
    "        transformed[('label', '')],\n",
    "        test_size=.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(\n",
    "        eda.data.drop(('label', ''), axis=1).values,\n",
    "        eda.data[('label', '')],\n",
    "        test_size=.2,\n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "# estimator = SVR(kernel=\"linear\")\n",
    "# selector = RFE(estimator, 3, step=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# transformed.drop(('label', ''), axis=1).columns.to_flat_index()[selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "pipe = Pipeline([\n",
    "    # the reduce_dim stage is populated by the param_grid\n",
    "    ('reduce_dim', 'passthrough'),\n",
    "    ('classify', LinearSVC(dual=False, max_iter=10000))\n",
    "])\n",
    "\n",
    "N_FEATURES_OPTIONS = [3, 5, 8, 15]\n",
    "C_OPTIONS = [1, 10, 100, 1000]\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(iterated_power=7), NMF()],\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest(chi2)],\n",
    "        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "]\n",
    "reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']\n",
    "\n",
    "grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X, y)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "# scores are in the order of param_grid iteration, which is alphabetical\n",
    "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "# select score for best C\n",
    "mean_scores = mean_scores.max(axis=0)\n",
    "bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n",
    "               (len(reducer_labels) + 1) + .5)\n",
    "\n",
    "plt.figure()\n",
    "COLORS = 'bgrcmyk'\n",
    "for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n",
    "    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n",
    "\n",
    "plt.title(\"Comparing feature reduction techniques\")\n",
    "plt.xlabel('Reduced number of features')\n",
    "plt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)\n",
    "plt.ylabel('Digit classification accuracy')\n",
    "plt.ylim((0, 1))\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}